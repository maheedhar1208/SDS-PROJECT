{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OJF9-lXOsEb",
        "outputId": "18e6c0fa-e2cd-46da-9f51-eb4689967b86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-12-15 04:42:31--  https://www.nuscenes.org/data/v1.0-mini.tgz\n",
            "Resolving www.nuscenes.org (www.nuscenes.org)... 18.64.174.111, 18.64.174.12, 18.64.174.116, ...\n",
            "Connecting to www.nuscenes.org (www.nuscenes.org)|18.64.174.111|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4167696325 (3.9G) [application/x-tar]\n",
            "Saving to: ‘v1.0-mini.tgz’\n",
            "\n",
            "v1.0-mini.tgz       100%[===================>]   3.88G  37.2MB/s    in 1m 40s  \n",
            "\n",
            "2022-12-15 04:44:12 (39.6 MB/s) - ‘v1.0-mini.tgz’ saved [4167696325/4167696325]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p /data/sets/nuscenes  # Make the directory to store the nuScenes dataset in.\n",
        "\n",
        "!wget https://www.nuscenes.org/data/v1.0-mini.tgz  # Download the nuScenes mini split.\n",
        "\n",
        "!tar -xf v1.0-mini.tgz -C /data/sets/nuscenes  # Uncompress the nuScenes mini split.\n",
        "\n",
        "!pip install nuscenes-devkit &> /dev/null  # Install nuScenes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnt6mlQ-Os23",
        "outputId": "2c45720a-7715-4e7f-a4a0-f654ad344d67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======\n",
            "Loading NuScenes tables for version v1.0-mini...\n",
            "23 category,\n",
            "8 attribute,\n",
            "4 visibility,\n",
            "911 instance,\n",
            "12 sensor,\n",
            "120 calibrated_sensor,\n",
            "31206 ego_pose,\n",
            "8 log,\n",
            "10 scene,\n",
            "404 sample,\n",
            "31206 sample_data,\n",
            "18538 sample_annotation,\n",
            "4 map,\n",
            "Done loading in 0.734 seconds.\n",
            "======\n",
            "Reverse indexing ...\n",
            "Done reverse indexing in 0.2 seconds.\n",
            "======\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "from nuscenes.nuscenes import NuScenes\n",
        "\n",
        "nusc = NuScenes(version='v1.0-mini', dataroot='/data/sets/nuscenes', verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SiFVs0PkJOr"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "schema1=StructType([StructField('token',StringType(),True),StructField('log_token',StringType(),True),StructField('nbr_samples',StringType(),True),StructField('first_sample_token',StringType(),True),StructField('last_sample_token',StringType(),True),StructField('name',StringType(),True),StructField('description',StringType(),True)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Q68KNj_ArsSy",
        "outputId": "7d54c6fc-80c5-4346-bb67-f2ac3dc744a3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/spark-3.2.3-bin-hadoop3.2'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0VIHco2oGJS"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Nuscenes1\")\\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2vpS3iQja4U"
      },
      "source": [
        "            ********Storing the data into csv files *********************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA9k_IAVjXYX"
      },
      "source": [
        "Scene"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JL6z7hwmwOOK"
      },
      "outputs": [],
      "source": [
        "list_scenes=list(nusc.scene)\n",
        "field_names= [i for i in list_scenes[0].keys()]\n",
        "import csv\n",
        "for i in range(0,1):\n",
        "  with open('scenes/scenes'+str(i+1)+'.csv', 'w') as csvfile:\n",
        "      writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
        "      writer.writeheader()\n",
        "      writer.writerows(list_scenes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbMot-pTHdAy"
      },
      "source": [
        "sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6j5LKNLBwIMc"
      },
      "outputs": [],
      "source": [
        "sample_lst=[]\n",
        "for i in nusc.sample:\n",
        "  sample_lst.append(i)\n",
        "sample_lst[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dySXBULTIJ3T",
        "outputId": "d6412b4b-f9a7-4e71-9027-51805eb67732"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "404"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "field_names_sample= [i for i in sample_lst[0].keys()]\n",
        "#field_names_sample\n",
        "len(sample_lst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z33RcnfKnKPA"
      },
      "outputs": [],
      "source": [
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdyYEXFVIRyJ"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "for i in range(0,1):\n",
        "  with open('samples/sample'+str(i+1)+'.csv', 'w') as csvfile:\n",
        "      writer = csv.DictWriter(csvfile, fieldnames=field_names_sample)\n",
        "      writer.writeheader()\n",
        "      if (i==3):\n",
        "        writer.writerows(sample_lst[300:404])\n",
        "      else:\n",
        "        writer.writerows(sample_lst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH_-ewL0JB9U"
      },
      "source": [
        "sample_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyRKfy6uJAw-",
        "outputId": "41b43b4e-0330-40cb-ba3e-31e94cdaf35f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'token': '5ace90b379af485b9dcb1584b01e7212',\n",
              " 'sample_token': '39586f9d59004284a7114a68825e8eec',\n",
              " 'ego_pose_token': '5ace90b379af485b9dcb1584b01e7212',\n",
              " 'calibrated_sensor_token': 'f4d2a6c281f34a7eb8bb033d82321f79',\n",
              " 'timestamp': 1532402927814384,\n",
              " 'fileformat': 'pcd',\n",
              " 'is_key_frame': False,\n",
              " 'height': 0,\n",
              " 'width': 0,\n",
              " 'filename': 'sweeps/RADAR_FRONT/n015-2018-07-24-11-22-45+0800__RADAR_FRONT__1532402927814384.pcd',\n",
              " 'prev': 'f0b8593e08594a3eb1152c138b312813',\n",
              " 'next': '978db2bcdf584b799c13594a348576d2',\n",
              " 'sensor_modality': 'radar',\n",
              " 'channel': 'RADAR_FRONT'}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_data_lst=[]\n",
        "for i in nusc.sample_data:\n",
        "  sample_data_lst.append(i)\n",
        "sample_data_lst[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rsyeXdHJud5"
      },
      "outputs": [],
      "source": [
        "field_names_sample_data= [i for i in sample_data_lst[0].keys()]\n",
        "#field_names_sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3A1mT8aJ9KB"
      },
      "outputs": [],
      "source": [
        "for i in range(0,1):\n",
        "  with open('sample_data1/sample_data'+str(i+1)+'.csv', 'w') as csvfile:\n",
        "      writer = csv.DictWriter(csvfile, fieldnames=field_names_sample_data)\n",
        "      writer.writeheader()\n",
        "    \n",
        "      writer.writerows(sample_data_lst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld8A4i-9MLX3"
      },
      "source": [
        "sample_annotation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBFYXfFsMKvN",
        "outputId": "06791113-2cbf-4432-f093-c00d9af3d5fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'token': '70aecbe9b64f4722ab3c230391a3beb8',\n",
              " 'sample_token': 'cd21dbfc3bd749c7b10a5c42562e0c42',\n",
              " 'instance_token': '6dd2cbf4c24b4caeb625035869bca7b5',\n",
              " 'visibility_token': '4',\n",
              " 'attribute_tokens': ['4d8821270b4a47e3a8a300cbec48188e'],\n",
              " 'translation': [373.214, 1130.48, 1.25],\n",
              " 'size': [0.621, 0.669, 1.642],\n",
              " 'rotation': [0.9831098797903927, 0.0, 0.0, -0.18301629506281616],\n",
              " 'prev': 'a1721876c0944cdd92ebc3c75d55d693',\n",
              " 'next': '1e8e35d365a441a18dd5503a0ee1c208',\n",
              " 'num_lidar_pts': 5,\n",
              " 'num_radar_pts': 0,\n",
              " 'category_name': 'human.pedestrian.adult'}"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_annotation_lst=[]\n",
        "for i in nusc.sample_annotation:\n",
        "  sample_annotation_lst.append(i)\n",
        "sample_annotation_lst[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geIRPEiXNDcH",
        "outputId": "f65d5751-41ee-4d53-d323-eec5f6b46bd2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "18538"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "field_names_sample_annotation= [i for i in sample_annotation_lst[0].keys()]\n",
        "len(sample_annotation_lst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROR1CMdqNKU-"
      },
      "outputs": [],
      "source": [
        "for i in range(0,2):\n",
        "  with open('sample_annotation/sample_annotation'+str(i+1)+'.csv', 'w') as csvfile:\n",
        "      writer = csv.DictWriter(csvfile, fieldnames=field_names_sample_annotation)\n",
        "      writer.writeheader()\n",
        "    \n",
        "      writer.writerows(sample_annotation_lst[i*9269:i*9269+9269])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O405czoN767"
      },
      "source": [
        "instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqTYHe3JOAUE",
        "outputId": "570e8b2e-bbca-4cf7-9f01-5bf4c582ec5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "911"
            ]
          },
          "execution_count": 149,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "instance=[]\n",
        "for i in nusc.instance:\n",
        "  instance.append(i)\n",
        "len(instance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y082wCHgOLGb",
        "outputId": "ed14f82b-819b-4408-80f0-c664aa8d0d2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "911"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "field_names_instance= [i for i in instance[0].keys()]\n",
        "len(instance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "9ApPtr6EORPf"
      },
      "outputs": [],
      "source": [
        "for i in range(0,2):\n",
        "  with open('instance/instance'+str(i+1)+'.csv', 'w') as csvfile:\n",
        "      writer = csv.DictWriter(csvfile, fieldnames=field_names_instance)\n",
        "      writer.writeheader()\n",
        "      if (i==0):\n",
        "       writer.writerows(instance[0:500])\n",
        "      else:\n",
        "         writer.writerows(instance[500:911])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIYwjxGCOqx9"
      },
      "source": [
        "category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZmaSrFDOkiP",
        "outputId": "96e846fa-8d98-4200-8c99-7dad409e63c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "category=[]\n",
        "for i in nusc.category:\n",
        "  category.append(i)\n",
        "len(category)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4hUtyzIOfwh"
      },
      "outputs": [],
      "source": [
        "field_names_category=[i for i in category[0].keys()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "lBNFK4ViO-3b",
        "outputId": "fdf1d403-c301-44a8-88f1-8c883d4cadb8"
      },
      "outputs": [],
      "source": [
        "with open('category/category'+str(1)+'.csv', 'w') as csvfile:\n",
        "      writer = csv.DictWriter(csvfile, fieldnames=field_names_category)\n",
        "      writer.writeheader()\n",
        "    \n",
        "      writer.writerows(category)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETz0dXI8PMS1"
      },
      "source": [
        "attribute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ba42LeuRPHaB",
        "outputId": "64c8b0c5-a7dc-4f9e-b9d1-b2b410eb28bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "attribute=[]\n",
        "for i in nusc.attribute:\n",
        "  attribute.append(i)\n",
        "len(attribute)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GnXnTG6PXnN"
      },
      "outputs": [],
      "source": [
        "field_names_attribute=[i for i in attribute[0].keys()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIym59EUPg_k"
      },
      "outputs": [],
      "source": [
        "with open('attribute/attribute'+str(1)+'.csv', 'w') as csvfile:\n",
        "      writer = csv.DictWriter(csvfile, fieldnames=field_names_attribute)\n",
        "      writer.writeheader()\n",
        "    \n",
        "      writer.writerows(attribute)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm3LsFXsPuA3"
      },
      "source": [
        "Visibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSazkiucPnI4",
        "outputId": "496febe9-bda8-4e37-fb97-7f4ac6698797"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "visibility=[]\n",
        "for i in nusc.visibility:\n",
        "  visibility.append(i)\n",
        "len(visibility)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_CCGzU3QLiU"
      },
      "outputs": [],
      "source": [
        "field_names_visibility=[i for i in visibility[0].keys()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3utluiMdQS9C"
      },
      "outputs": [],
      "source": [
        "with open('visibility/visibility'+str(1)+'.csv', 'w') as csvfile:\n",
        "      writer = csv.DictWriter(csvfile, fieldnames=field_names_visibility)\n",
        "      writer.writeheader()\n",
        "    \n",
        "      writer.writerows(visibility)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK_2Ftk1Qd_o"
      },
      "source": [
        "sensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0PP0LK4QcpO",
        "outputId": "89e5e8cc-6458-499c-a1a4-10d7f6d2a059"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sensor=[]\n",
        "for i in nusc.sensor:\n",
        "  sensor.append(i)\n",
        "len(sensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAG2SOriQ7M6"
      },
      "outputs": [],
      "source": [
        "field_names_sensor=[i for i in sensor[0].keys()]\n",
        "with open('sensor/sensor'+str(1)+'.csv', 'w') as csvfile:\n",
        "      writer = csv.DictWriter(csvfile, fieldnames=field_names_sensor)\n",
        "      writer.writeheader()\n",
        "    \n",
        "      writer.writerows(sensor)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD66J17CRHhD"
      },
      "source": [
        "calibrated_sensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Az6cVC0RGVX",
        "outputId": "2643ed51-bc44-488c-8aa6-79d9c74bcf4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "120"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "c_sensor=[]\n",
        "for i in nusc.calibrated_sensor:\n",
        "   c_sensor.append(i)\n",
        "len(c_sensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIrpVNvnRVcx"
      },
      "outputs": [],
      "source": [
        "field_names_c_sensor=[i for i in c_sensor[0].keys()]\n",
        "with open('c_sensor/c_sensor'+str(1)+'.csv', 'w') as csvfile:\n",
        "      writer = csv.DictWriter(csvfile, fieldnames=field_names_c_sensor)\n",
        "      writer.writeheader()\n",
        "    \n",
        "      writer.writerows(c_sensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_9qA-aORlbt"
      },
      "source": [
        "ego_pose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGE0FEYlRjlX",
        "outputId": "f5c56518-48a3-4059-a3fc-0404b12a0496"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "31206"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ego_pose=[]\n",
        "for i in nusc.ego_pose:\n",
        "  ego_pose.append(i)\n",
        "len(ego_pose)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tsbo-SLOSNOt"
      },
      "outputs": [],
      "source": [
        "field_names_ego_pose=[i for i in ego_pose[0].keys()]\n",
        "with open('ego_pose/ego_pose'+str(1)+'.csv', 'w') as csvfile:\n",
        "      writer = csv.DictWriter(csvfile, fieldnames=field_names_ego_pose)\n",
        "      writer.writeheader()\n",
        "    \n",
        "      writer.writerows(ego_pose)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv65zK2-SX1t"
      },
      "source": [
        "log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z06uYXiESWzi",
        "outputId": "12539f00-3c1e-481d-ded4-63b8788cd71f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "execution_count": 176,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "log=[]\n",
        "for i in nusc.log:\n",
        "  log.append(i)\n",
        "len(log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "rYAKc0V4SfO5"
      },
      "outputs": [],
      "source": [
        "field_names_log=[i for i in log[0].keys()]\n",
        "for i in range(0,2):\n",
        "  with open('log/log'+str(1)+'.csv', 'w') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=field_names_log)\n",
        "        writer.writeheader()\n",
        "      \n",
        "        writer.writerows(log[4*i:4*i+4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFAYmyajSoPv"
      },
      "source": [
        "Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnX558YkSmwj",
        "outputId": "46ad1dfa-3349-4138-9cfd-7f176f1d0883"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "map=[]\n",
        "for i in nusc.map:\n",
        "  map.append(i)\n",
        "len(map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwV0hZy4Suwm"
      },
      "outputs": [],
      "source": [
        "field_names_map=[i for i in map[0].keys()]\n",
        "with open('map/map'+str(1)+'.csv', 'w') as csvfile:\n",
        "      writer = csv.DictWriter(csvfile, fieldnames=field_names_map)\n",
        "      writer.writeheader()\n",
        "    \n",
        "      writer.writerows(map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KVh51A5joqX"
      },
      "source": [
        "        ***************Queries uisng pyspark streaming *********************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0Wj1tdUasI3"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsrOprdwka_L"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tMmBzT7YIDe"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Query1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVtvqVO9Z5L3"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "schema1=StructType([StructField('token',StringType(),True),StructField('sample_token',StringType(),True),StructField('instance_token',StringType(),True),StructField('visibility_token',StringType(),True),StructField('attribute_tokens',StringType(),True),StructField('translation',StringType(),True),StructField('size',StringType(),True),StructField('rotation',StringType(),True),StructField('prev',StringType(),True),StructField('next',StringType(),True),StructField('num_lidar_pts',StringType(),True),StructField('num_radar_pts',StringType(),True),StructField('category_name',StringType(),True)])\n",
        "scene=spark.readStream.format(\"csv\").schema(schema1).option(\"header\",True).option(\"maxFilesPerTrigger\",1).load(\"sample_annotation\")\n",
        "query22=scene.groupBy(\"category_name\").count()\n",
        "q=query22.writeStream.queryName(\"query22_6\").format(\"memory\").outputMode(\"complete\").start()\n",
        "for i in range(0,5):\n",
        "  _df=spark.sql(\"SELECT * FROM query22_6\")\n",
        "  _df.show()\n",
        "  time.sleep(2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Query2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GAjb5Ce8F4V"
      },
      "outputs": [],
      "source": [
        "\n",
        "schema2=StructType([StructField('token',StringType(),True),StructField('sample_token',StringType(),True),StructField('instance_token',StringType(),True),StructField('visibility_token',StringType(),True),StructField('attribute_tokens',StringType(),True),StructField('translation',StringType(),True),StructField('size',StringType(),True),StructField('rotation',StringType(),True),StructField('prev',StringType(),True),StructField('next',StringType(),True),StructField('num_lidar_pts',StringType(),True),StructField('num_radar_pts',StringType(),True),StructField('category_name',StringType(),True)])\n",
        "scene=spark.readStream.format(\"csv\").schema(schema2).option(\"header\",True).option(\"maxFilesPerTrigger\",1).load(\"sample_annotation\")\n",
        "query23=scene.select(\"category_name\")\n",
        "q=query23.writeStream.queryName(\"query22_7\").format(\"memory\").outputMode(\"append\").start()\n",
        "for i in range(0,5):\n",
        "  _df=spark.sql(\"SELECT DISTINCT(category_name) FROM query22_7\")\n",
        "  _df.show()\n",
        "  time.sleep(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Query3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "CrFidx6sJyl0"
      },
      "outputs": [],
      "source": [
        "\n",
        "schema3=StructType([StructField('token',StringType(),True),StructField('sample_token',StringType(),True),StructField('instance_token',StringType(),True),StructField('visibility_token',StringType(),True),StructField('attribute_tokens',StringType(),True),StructField('translation',StringType(),True),StructField('size',StringType(),True),StructField('rotation',StringType(),True),StructField('prev',StringType(),True),StructField('next',StringType(),True),StructField('num_lidar_pts',StringType(),True),StructField('num_radar_pts',StringType(),True),StructField('category_name',StringType(),True)])\n",
        "scene=spark.readStream.format(\"csv\").schema(schema3).option(\"header\",True).option(\"maxFilesPerTrigger\",1).load(\"sample_annotation\")\n",
        "query24=scene.select(\"category_name\").where(scene['category_name']== 'human.pedestrian.child')\n",
        "q=query24.writeStream.queryName(\"query22_27\").format(\"memory\").outputMode(\"append\").start()\n",
        "for i in range(0,5):\n",
        "  _df=spark.sql(\"SELECT COUNT(category_name) FROM query22_27\")\n",
        "  _df.show()\n",
        "  time.sleep(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Query4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "-sVWyRqjnaHT"
      },
      "outputs": [],
      "source": [
        "\n",
        "schema4=StructType([StructField('token',StringType(),True),StructField('log_token',StringType(),True),StructField('nbr_samples',StringType(),True),StructField('first_sample_token',StringType(),True),StructField('last_sample_token',StringType(),True),StructField('name',StringType(),True),StructField('description',StringType(),True)])\n",
        "scene=spark.readStream.format(\"csv\").schema(schema4).option(\"header\",True).option(\"maxFilesPerTrigger\",1).load(\"scenes\")\n",
        "query24=scene.select(\"nbr_samples\")\n",
        "q=query24.writeStream.queryName(\"query22_28\").format(\"memory\").outputMode(\"append\").start()\n",
        "for i in range(0,5):\n",
        "  _df=spark.sql(\"SELECT SUM(nbr_samples) FROM query22_28\")\n",
        "  _df.show()\n",
        "  time.sleep(2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Query5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "iJbWqwJMo7hk"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "schema5=StructType([StructField('token',StringType(),True),StructField('sample_token',StringType(),True),StructField('ego_pose_token',StringType(),True),StructField('calibrated_sensor_token',StringType(),True),StructField('timestamp',IntegerType(),True),StructField('fileformat',StringType(),True),StructField('is_key_frame',StringType(),True),StructField('height',IntegerType(),True),StructField('width',IntegerType(),True),StructField('filename',StringType(),True),StructField('prev',StringType(),True),StructField('next',StringType(),True),StructField('sensor_modality',StringType(),True),StructField('channel',StringType(),True)])\n",
        "scene=spark.readStream.format(\"csv\").schema(schema5).option(\"header\",True).option(\"maxFilesPerTrigger\",1).load(\"sample_data1\")\n",
        "query24=scene.groupBy(\"channel\").count()\n",
        "q=query24.writeStream.queryName(\"query22_29\").format(\"memory\").outputMode(\"complete\").start()\n",
        "for i in range(0,5):\n",
        "  _df=spark.sql(\"SELECT * FROM query22_29\")\n",
        "  _df.show()\n",
        "  time.sleep(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Query6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "w7w3OGEnp-6G"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "schema6=StructType([StructField('token',StringType(),True),StructField('sample_token',StringType(),True),StructField('ego_pose_token',StringType(),True),StructField('calibrated_sensor_token',StringType(),True),StructField('timestamp',IntegerType(),True),StructField('fileformat',StringType(),True),StructField('is_key_frame',StringType(),True),StructField('height',IntegerType(),True),StructField('width',IntegerType(),True),StructField('filename',StringType(),True),StructField('prev',StringType(),True),StructField('next',StringType(),True),StructField('sensor_modality',StringType(),True),StructField('channel',StringType(),True)])\n",
        "scene=spark.readStream.format(\"csv\").schema(schema6).option(\"header\",True).option(\"maxFilesPerTrigger\",1).load(\"sample_data1\")\n",
        "query24=scene.select(\"channel\").where(scene['channel'] == 'RADAR_FRONT')\n",
        "q=query24.writeStream.queryName(\"query22_32\").format(\"memory\").outputMode(\"append\").start()\n",
        "for i in range(0,5):\n",
        "  _df=spark.sql(\"SELECT COUNT(channel) FROM query22_32\")\n",
        "  _df.show()\n",
        "  time.sleep(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Query7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "ilvDPzdo27UW"
      },
      "outputs": [],
      "source": [
        "\n",
        "schema6=StructType([StructField('token',StringType(),True),StructField('sample_token',StringType(),True),StructField('instance_token',StringType(),True),StructField('visibility_token',StringType(),True),StructField('attribute_tokens',StringType(),True),StructField('translation',StringType(),True),StructField('size',StringType(),True),StructField('rotation',StringType(),True),StructField('prev',StringType(),True),StructField('next',StringType(),True),StructField('num_lidar_pts',StringType(),True),StructField('num_radar_pts',StringType(),True),StructField('category_name',StringType(),True)])\n",
        "scene=spark.readStream.format(\"csv\").schema(schema6).option(\"header\",True).option(\"maxFilesPerTrigger\",1).load(\"sample_annotation\")\n",
        "query24=scene.groupBy(\"visibility_token\").count()\n",
        "q=query24.writeStream.queryName(\"query22_33\").format(\"memory\").outputMode(\"complete\").start()\n",
        "for i in range(0,5):\n",
        "  _df=spark.sql(\"SELECT * FROM query22_33\")\n",
        "  _df.show()\n",
        "  time.sleep(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Query8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "bA1KUyAgLfa8"
      },
      "outputs": [],
      "source": [
        "\n",
        "schema7=StructType([StructField('token',StringType(),True),StructField('sample_token',StringType(),True),StructField('instance_token',StringType(),True),StructField('visibility_token',StringType(),True),StructField('attribute_tokens',StringType(),True),StructField('translation',StringType(),True),StructField('size',StringType(),True),StructField('rotation',StringType(),True),StructField('prev',StringType(),True),StructField('next',StringType(),True),StructField('num_lidar_pts',StringType(),True),StructField('num_radar_pts',StringType(),True),StructField('category_name',StringType(),True)])\n",
        "scene=spark.readStream.format(\"csv\").schema(schema6).option(\"header\",True).option(\"maxFilesPerTrigger\",1).load(\"sample_annotation\")\n",
        "query24=scene.select(\"visibility_token\").where(scene['visibility_token']== 2)\n",
        "q=query24.writeStream.queryName(\"query22_34\").format(\"memory\").outputMode(\"append\").start()\n",
        "for i in range(0,5):\n",
        "  _df=spark.sql(\"SELECT COUNT(visibility_token) FROM query22_34\")\n",
        "  _df.show()\n",
        "  time.sleep(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Query9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "0lQk680vNpXJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "schema8=StructType([StructField('token',StringType(),True),StructField('name',StringType(),True),StructField('description',StringType(),True)])\n",
        "scene=spark.readStream.format(\"csv\").schema(schema8).option(\"header\",True).option(\"maxFilesPerTrigger\",1).load(\"instance\")\n",
        "query24=scene.groupBy(\"name\").count()\n",
        "q=query24.writeStream.queryName(\"query22_36\").format(\"memory\").outputMode(\"complete\").start()\n",
        "for i in range(0,5):\n",
        "  time.sleep(10)\n",
        "  _df=spark.sql(\"SELECT * FROM query22_36\")\n",
        "  _df.show()\n",
        "  time.sleep(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Query10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "ZKzet0OCUsz5"
      },
      "outputs": [],
      "source": [
        "\n",
        "schema9=StructType([StructField('token',StringType(),True),StructField('logfile',StringType(),True),StructField('vehicle',StringType(),True),StructField('date_captured',StringType(),True),StructField('location',StringType(),True),StructField('map_token',StringType(),True)])\n",
        "scene=spark.readStream.format(\"csv\").schema(schema9).option(\"header\",True).option(\"maxFilesPerTrigger\",1).load(\"log\")\n",
        "query24=scene.select(\"location\")\n",
        "q=query24.writeStream.queryName(\"query22_38\").format(\"memory\").outputMode(\"append\").start()\n",
        "for i in range(0,5):\n",
        "  _df=spark.sql(\"SELECT DISTINCT(location) FROM query22_38\")\n",
        "  _df.show()\n",
        "  time.sleep(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Query11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "eky5xfXJYLjx"
      },
      "outputs": [],
      "source": [
        "\n",
        "schema10=StructType([StructField('token',StringType(),True),StructField('log_token',StringType(),True),StructField('nbr_samples',StringType(),True),StructField('first_sample_token',StringType(),True),StructField('last_sample_token',StringType(),True),StructField('name',StringType(),True),StructField('description',StringType(),True)])\n",
        "scene=spark.readStream.format(\"csv\").schema(schema10).option(\"header\",True).option(\"maxFilesPerTrigger\",1).load(\"scenes\")\n",
        "query24=scene.select(\"nbr_samples\")\n",
        "q=query24.writeStream.queryName(\"query22_39\").format(\"memory\").outputMode(\"append\").start()\n",
        "for i in range(0,5):\n",
        "  _df=spark.sql(\"SELECT AVG(nbr_samples) FROM query22_39\")\n",
        "  _df.show()\n",
        "  time.sleep(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Query12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "LDgFrVP8Zyql"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "schema11=StructType([StructField('token',StringType(),True),StructField('sample_token',StringType(),True),StructField('ego_pose_token',StringType(),True),StructField('calibrated_sensor_token',StringType(),True),StructField('timestamp',IntegerType(),True),StructField('fileformat',StringType(),True),StructField('is_key_frame',StringType(),True),StructField('height',IntegerType(),True),StructField('width',IntegerType(),True),StructField('filename',StringType(),True),StructField('prev',StringType(),True),StructField('next',StringType(),True),StructField('sensor_modality',StringType(),True),StructField('channel',StringType(),True)])\n",
        "scene=spark.readStream.format(\"csv\").schema(schema11).option(\"header\",True).option(\"maxFilesPerTrigger\",1).load(\"sample_data1\")\n",
        "query24=scene.groupBy('sensor_modality').count()\n",
        "q=query24.writeStream.queryName(\"query22_40\").format(\"memory\").outputMode(\"complete\").start()\n",
        "for i in range(0,5):\n",
        "  _df=spark.sql(\"SELECT * FROM query22_40\")\n",
        "  _df.show()\n",
        "  time.sleep(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Query13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "id": "jN-du84HgsJF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "schema12=StructType([StructField('token',StringType(),True),StructField('sample_token',StringType(),True),StructField('ego_pose_token',StringType(),True),StructField('calibrated_sensor_token',StringType(),True),StructField('timestamp',IntegerType(),True),StructField('fileformat',StringType(),True),StructField('is_key_frame',StringType(),True),StructField('height',IntegerType(),True),StructField('width',IntegerType(),True),StructField('filename',StringType(),True),StructField('prev',StringType(),True),StructField('next',StringType(),True),StructField('sensor_modality',StringType(),True),StructField('channel',StringType(),True)])\n",
        "scene=spark.readStream.format(\"csv\").schema(schema12).option(\"header\",True).option(\"maxFilesPerTrigger\",1).load(\"sample_data1\")\n",
        "query24=scene.select('sensor_modality')\n",
        "q=query24.writeStream.queryName(\"query22_41\").format(\"memory\").outputMode(\"append\").start()\n",
        "for i in range(0,5):\n",
        "  _df=spark.sql(\"SELECT DISTINCT(sensor_modality) FROM query22_41\")\n",
        "  _df.show()\n",
        "  time.sleep(10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
